{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  Transformers and Accelerate for model loading\n",
        "!pip install -q transformers==4.37.2 accelerate==0.26.1\n",
        "\n",
        "# âš™ï¸ Required for LLaVA (vision + language)\n",
        "!pip install -q sentencepiece git+https://github.com/huggingface/peft.git\n",
        "\n",
        "# ğŸ“„ For reading/writing .docx files\n",
        "!pip install -q python-docx\n",
        "\n",
        "\n",
        "# ğŸ–¼ï¸ For image handling\n",
        "!pip install -q pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h4Ofpd3JC_C",
        "outputId": "4d912bae-eda5-4d69-c4ac-19d3efe0533f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT2XzbOVIVJy",
        "outputId": "d4719b8a-7b0f-4b35-ee8b-94ce9baa561a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¼ï¸ page04_img01.png â†’ ğŸ“Œ Part 15\n",
            "ğŸ–¼ï¸ page13_img01.png â†’ ğŸ“Œ Part 2\n",
            "ğŸ–¼ï¸ page06_img01.png â†’ ğŸ“Œ Part 11\n",
            "ğŸ–¼ï¸ page05_img01.png â†’ ğŸ“Œ Part 15\n",
            "ğŸ–¼ï¸ page01_img01.png â†’ ğŸ“Œ Part 15\n",
            "ğŸ–¼ï¸ page03_img01.png â†’ ğŸ“Œ Part 15\n",
            "ğŸ–¼ï¸ page14_img01.png â†’ ğŸ“Œ Part 1\n",
            "ğŸ–¼ï¸ page19_img01.png â†’ ğŸ“Œ Part 5\n",
            "ğŸ–¼ï¸ page07_img01.png â†’ ğŸ“Œ Part 15\n",
            "ğŸ–¼ï¸ page12_img01.png â†’ ğŸ“Œ Part 10\n",
            "âœ… New lecture notes saved to /content/lecture_notes_with_images.docx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# --- Paths ---\n",
        "NOTES_PATH = \"/content/lecture_notes.docx\"\n",
        "IMAGES_DIR = \"/content/images\"\n",
        "OUTPUT_DOCX = \"/content/lecture_notes_with_images.docx\"\n",
        "\n",
        "# --- Load CLIP model and processor ---\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "# --- Load docx notes ---\n",
        "doc = Document(NOTES_PATH)\n",
        "paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
        "\n",
        "# --- Combine paragraphs into sections by heading ---\n",
        "sections = []\n",
        "current_section = {\"title\": \"Introduction\", \"content\": \"\"}\n",
        "for p in paragraphs:\n",
        "    if p.startswith(\"Part\") or p.lower().startswith(\"section\") or p.startswith(\"##\"):\n",
        "        sections.append(current_section)\n",
        "        current_section = {\"title\": p, \"content\": \"\"}\n",
        "    else:\n",
        "        current_section[\"content\"] += p + \"\\n\"\n",
        "sections.append(current_section)\n",
        "\n",
        "# --- Function to encode both text and image ---\n",
        "def encode_text_and_image(text, image_path):\n",
        "    # Open image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Preprocess text and image using CLIPProcessor, this automatically handles truncation\n",
        "    inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Get embeddings for both text and image\n",
        "    text_features = clip_model.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "    image_features = clip_model.get_image_features(pixel_values=inputs['pixel_values'])\n",
        "\n",
        "    return text_features, image_features\n",
        "\n",
        "# --- Function to determine best section for image ---\n",
        "def find_best_section_for_image(image_path, sections):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    scores = []\n",
        "\n",
        "    for section in sections:\n",
        "        prompt = f\"Which section best matches this image?\\n\\nSection:\\n{section['content'][:500]}\"\n",
        "\n",
        "        # Truncate the prompt text to ensure it fits within the CLIP model's input size\n",
        "        # Tokenize the text manually before encoding\n",
        "        inputs = clip_processor(text=prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Get features for text and image\n",
        "        text_features, image_features = encode_text_and_image(prompt, image_path)\n",
        "\n",
        "        # Calculate similarity (cosine similarity between text and image features)\n",
        "        similarity = torch.cosine_similarity(text_features, image_features)\n",
        "\n",
        "        # Store the section with the highest similarity score\n",
        "        scores.append((section[\"title\"], similarity.item()))\n",
        "\n",
        "    if scores:\n",
        "        # Sort by similarity score and return the section with the highest score\n",
        "        best_section = max(scores, key=lambda x: x[1])\n",
        "        return best_section[0]\n",
        "    else:\n",
        "        return \"Introduction\"\n",
        "\n",
        "# --- Map images to best-fit section ---\n",
        "image_placement_map = {}\n",
        "for img_file in os.listdir(IMAGES_DIR):\n",
        "    if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\")):\n",
        "        image_path = os.path.join(IMAGES_DIR, img_file)\n",
        "        best_section = find_best_section_for_image(image_path, sections)\n",
        "        image_placement_map[best_section] = image_path\n",
        "        print(f\"ğŸ–¼ï¸ {img_file} â†’ ğŸ“Œ {best_section}\")\n",
        "\n",
        "# --- Create new docx with images inserted ---\n",
        "new_doc = Document()\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    new_doc.add_paragraph(para.text, style=para.style)\n",
        "\n",
        "    if para.text.strip() in image_placement_map:\n",
        "        image_path = image_placement_map[para.text.strip()]\n",
        "        new_doc.add_picture(image_path, width=Inches(5))\n",
        "        new_doc.add_paragraph(\"\")  # spacing\n",
        "\n",
        "new_doc.save(OUTPUT_DOCX)\n",
        "print(f\"âœ… New lecture notes saved to {OUTPUT_DOCX}\")"
      ]
    }
  ]
}